{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de Seguridad (Asistente Multimodal) — Vídeo ➜ Vídeo + Voz\n",
    "\n",
    "**Objetivo del prototipo**  \n",
    "Procesar vídeos de cámaras de seguridad (`videos/entrada/*.mp4`) y, tras cada vídeo:\n",
    "\n",
    "- Generar un **vídeo de salida** con las **personas recuadradas** y, si corresponde, la **caja/paquete recuadrada**.\n",
    "- Mostrar un **print**:\n",
    "  - Si se detecta que una persona porta una caja/paquete: `Ha llegado un repartidor a tu casa`.\n",
    "  - Si no: `Ha llegado alguien desconocido a tu casa`.\n",
    "- Generar un **audio (TTS)** con un mensaje parecido:\n",
    "  - Repartidor: “Ha llegado un repartidor a tu domicilio, ¿deseas abrirle?”\n",
    "  - Desconocido: “Ha llegado alguien desconocido a tu domicilio, ¿deseas abrir?”\n",
    "\n",
    "**Notas de implementación**\n",
    "- Detección de personas con **YOLOv8**.\n",
    "- Detección de “caja/paquete” con **CLIP** aplicado a un recorte por persona (heurística ligera).\n",
    "- Salidas en `videos/salida/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instalando: ultralytics>=8.0.0 opencv-python pillow numpy transformers>=4.35.0 torch torchvision gTTS protobuf>=5.28.0\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Setup (instalación de dependencias)\n",
    "# Nota: si ya tienes un venv con todo, esta celda puede omitirse.\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    \"ultralytics>=8.0.0\",\n",
    "    \"opencv-python\",\n",
    "    \"pillow\",\n",
    "    \"numpy\",\n",
    "    \"transformers>=4.35.0\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"gTTS\",\n",
    "    # Evita errores de compatibilidad (p.ej. tensorflow -> protobuf)\n",
    "    \"protobuf>=5.28.0\",\n",
    "]\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + list(pkgs)\n",
    "    print(\"Instalando:\", \" \".join(pkgs))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "pip_install(packages)\n",
    "print(\"OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Input: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\entrada\n",
      "Output: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# IMPORTANTE: evitar que Transformers intente importar TensorFlow.\n",
    "# En Windows + Python 3.13 es frecuente que TF/protobuf rompan imports.\n",
    "import os\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from gtts import gTTS\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "VIDEOS_IN = ROOT / \"videos\" / \"entrada\"\n",
    "VIDEOS_OUT = ROOT / \"videos\" / \"salida\"\n",
    "VIDEOS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "print(\"Input:\", VIDEOS_IN)\n",
    "print(\"Output:\", VIDEOS_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos cargados\n"
     ]
    }
   ],
   "source": [
    "# Carga de modelos\n",
    "\n",
    "# YOLOv8 (detección)\n",
    "yolo = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# CLIP (clasificación binaria sobre la persona)\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
    "\n",
    "COCO_PERSON_CLASS_ID = 0\n",
    "\n",
    "# Prompts binarios (reducen falsos positivos respecto a buscar \"caja\" en un recorte pequeño)\n",
    "CLIP_LABELS = [\n",
    "    \"una persona sosteniendo un paquete o una caja de cartón\",  # positivo\n",
    "    \"una persona sin nada en las manos\",                       # negativo\n",
    "]\n",
    "\n",
    "CLIP_POSITIVE_INDEX = 0\n",
    "CLIP_NEGATIVE_INDEX = 1\n",
    "\n",
    "print(\"Modelos cargados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Box:\n",
    "    x1: int\n",
    "    y1: int\n",
    "    x2: int\n",
    "    y2: int\n",
    "\n",
    "    def clip(self, w: int, h: int) -> \"Box\":\n",
    "        return Box(\n",
    "            x1=max(0, min(self.x1, w - 1)),\n",
    "            y1=max(0, min(self.y1, h - 1)),\n",
    "            x2=max(0, min(self.x2, w - 1)),\n",
    "            y2=max(0, min(self.y2, h - 1)),\n",
    "        )\n",
    "\n",
    "    def valid(self) -> bool:\n",
    "        return self.x2 > self.x1 and self.y2 > self.y1\n",
    "\n",
    "    def area(self) -> int:\n",
    "        if not self.valid():\n",
    "            return 0\n",
    "        return (self.x2 - self.x1) * (self.y2 - self.y1)\n",
    "\n",
    "\n",
    "def draw_box(img_bgr: np.ndarray, box: Box, label: str, color: Tuple[int, int, int], thickness: int = 2) -> None:\n",
    "    cv2.rectangle(img_bgr, (box.x1, box.y1), (box.x2, box.y2), color, thickness)\n",
    "    if label:\n",
    "        (tw, th), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "        y = max(0, box.y1 - th - baseline - 4)\n",
    "        cv2.rectangle(img_bgr, (box.x1, y), (box.x1 + tw + 6, y + th + baseline + 6), color, -1)\n",
    "        cv2.putText(img_bgr, label, (box.x1 + 3, y + th + 3), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "\n",
    "\n",
    "def tts_to_mp3(text: str, out_path: Path, lang: str = \"es\") -> Path:\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(str(out_path))\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def clip_person_delivery_probs(pil_person: Image.Image) -> Tuple[float, float]:\n",
    "    \"\"\"Devuelve (p_pos, p_neg) usando CLIP_LABELS binarios.\"\"\"\n",
    "    inputs = clip_processor(text=CLIP_LABELS, images=pil_person, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        probs = outputs.logits_per_image.softmax(dim=1)[0]\n",
    "\n",
    "    p_pos = float(probs[CLIP_POSITIVE_INDEX].detach().cpu().item())\n",
    "    p_neg = float(probs[CLIP_NEGATIVE_INDEX].detach().cpu().item())\n",
    "    return p_pos, p_neg\n",
    "\n",
    "\n",
    "def person_carry_region(person: Box, frame_w: int, frame_h: int) -> Box:\n",
    "    # Región auxiliar (solo para visualizar / debug si hace falta)\n",
    "    w = person.x2 - person.x1\n",
    "    h = person.y2 - person.y1\n",
    "\n",
    "    x1 = int(person.x1 - 0.10 * w)\n",
    "    x2 = int(person.x2 + 0.10 * w)\n",
    "    y1 = int(person.y1 + 0.15 * h)\n",
    "    y2 = int(person.y1 + 0.95 * h)\n",
    "\n",
    "    return Box(x1, y1, x2, y2).clip(frame_w, frame_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(\n",
    "    frame_bgr: np.ndarray,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: float = 0.80,\n",
    "    clip_margin: float = 0.15,\n",
    "    min_person_area_ratio: float = 0.03,\n",
    ") -> Tuple[np.ndarray, bool]:\n",
    "    \"\"\"Devuelve (frame_anotado, delivery_detected_en_este_frame).\n",
    "\n",
    "    Cambios respecto a la versión anterior:\n",
    "    - CLIP binario sobre el recorte COMPLETO de la persona (menos falsos positivos)\n",
    "    - Requiere margen p_pos > p_neg + clip_margin\n",
    "    - Ignora personas muy pequeñas (ruido)\n",
    "    \"\"\"\n",
    "    annotated = frame_bgr.copy()\n",
    "    h, w = annotated.shape[:2]\n",
    "\n",
    "    result = yolo.predict(frame_bgr, conf=conf, verbose=False)[0]\n",
    "    if result.boxes is None or len(result.boxes) == 0:\n",
    "        return annotated, False\n",
    "\n",
    "    delivery = False\n",
    "    min_person_area = int(min_person_area_ratio * (w * h))\n",
    "\n",
    "    for box_xyxy, cls_id, score in zip(\n",
    "        result.boxes.xyxy.cpu().numpy(),\n",
    "        result.boxes.cls.cpu().numpy(),\n",
    "        result.boxes.conf.cpu().numpy(),\n",
    "    ):\n",
    "        if int(cls_id) != COCO_PERSON_CLASS_ID:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = [int(v) for v in box_xyxy]\n",
    "        person = Box(x1, y1, x2, y2).clip(w, h)\n",
    "        if not person.valid() or person.area() < min_person_area:\n",
    "            continue\n",
    "\n",
    "        # Clasificación CLIP sobre el recorte de la persona\n",
    "        person_crop_bgr = frame_bgr[person.y1 : person.y2, person.x1 : person.x2]\n",
    "        if person_crop_bgr.size == 0:\n",
    "            continue\n",
    "\n",
    "        person_crop_rgb = cv2.cvtColor(person_crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_person = Image.fromarray(person_crop_rgb)\n",
    "\n",
    "        p_pos, p_neg = clip_person_delivery_probs(pil_person)\n",
    "\n",
    "        # Criterio robusto (reduce falsos positivos)\n",
    "        is_delivery = (p_pos >= clip_threshold) and (p_pos > p_neg + clip_margin)\n",
    "\n",
    "        if is_delivery:\n",
    "            delivery = True\n",
    "            draw_box(annotated, person, f\"REPARTIDOR {score:.2f} | {p_pos:.2f}\", (0, 0, 255), 3)\n",
    "        else:\n",
    "            draw_box(annotated, person, f\"persona {score:.2f} | {p_pos:.2f}\", (0, 200, 0), 2)\n",
    "\n",
    "    return annotated, delivery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(\n",
    "    input_path: Path,\n",
    "    output_path: Path,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: float = 0.80,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> bool:\n",
    "    \"\"\"Procesa un vídeo y devuelve True si se detectó 'repartidor' en algún frame.\"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(str(input_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"No se pudo abrir el vídeo: {input_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if not fps or fps <= 0:\n",
    "        fps = 25.0\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "\n",
    "    delivery_any = False\n",
    "    frame_idx = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            annotated, delivery = process_frame(\n",
    "                frame,\n",
    "                conf=conf,\n",
    "                clip_threshold=clip_threshold,\n",
    "            )\n",
    "            delivery_any = delivery_any or delivery\n",
    "\n",
    "            out.write(annotated)\n",
    "\n",
    "            frame_idx += 1\n",
    "            if max_frames is not None and frame_idx >= max_frames:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "    return delivery_any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando: video_01.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_01.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_01.mp3\n",
      "\n",
      "Procesando: video_02.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_02.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_02.mp3\n",
      "\n",
      "Procesando: video_03.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_03.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_03.mp3\n",
      "\n",
      "Procesando: video_04.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_04.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_04.mp3\n",
      "\n",
      "Procesando: video_05.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_05.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_05.mp3\n"
     ]
    }
   ],
   "source": [
    "def run_on_folder(\n",
    "    input_dir: Path = VIDEOS_IN,\n",
    "    output_dir: Path = VIDEOS_OUT,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: float = 0.35,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> None:\n",
    "    videos = sorted(input_dir.glob(\"*.mp4\"))\n",
    "    if not videos:\n",
    "        print(f\"No hay vídeos .mp4 en {input_dir}\")\n",
    "        return\n",
    "\n",
    "    for in_path in videos:\n",
    "        out_video = output_dir / f\"output_{in_path.name}\"\n",
    "        out_audio = output_dir / f\"output_{in_path.stem}.mp3\"\n",
    "\n",
    "        print(\"\\nProcesando:\", in_path.name)\n",
    "        delivery = process_video(\n",
    "            input_path=in_path,\n",
    "            output_path=out_video,\n",
    "            conf=conf,\n",
    "            clip_threshold=clip_threshold,\n",
    "            max_frames=max_frames,\n",
    "        )\n",
    "\n",
    "        if delivery:\n",
    "            msg_print = \"Ha llegado un repartidor a tu casa\"\n",
    "            msg_tts = \"Ha llegado un repartidor a tu domicilio. ¿Deseas abrirle?\"\n",
    "        else:\n",
    "            msg_print = \"Ha llegado alguien desconocido a tu casa\"\n",
    "            msg_tts = \"Ha llegado alguien desconocido a tu domicilio. ¿Deseas abrir?\"\n",
    "\n",
    "        print(msg_print)\n",
    "        tts_to_mp3(msg_tts, out_audio)\n",
    "        print(\"Vídeo salida:\", out_video)\n",
    "        print(\"Audio salida:\", out_audio)\n",
    "\n",
    "\n",
    "# Ejecuta el pipeline sobre todos los vídeos de entrada.\n",
    "# Consejo: para pruebas rápidas, usa max_frames=150.\n",
    "run_on_folder(max_frames=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibración (negativo=video_01.mp4): max_p_pos=0.937 -> clip_threshold=0.950\n",
      "\n",
      "Procesando: video_01.mp4\n",
      "[1/5] Ha llegado alguien desconocido a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_01.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_01.mp3\n",
      "\n",
      "Procesando: video_02.mp4\n",
      "[2/5] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_02.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_02.mp3\n",
      "\n",
      "Procesando: video_03.mp4\n",
      "[3/5] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_03.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_03.mp3\n",
      "\n",
      "Procesando: video_04.mp4\n",
      "[4/5] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_04.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_04.mp3\n",
      "\n",
      "Procesando: video_05.mp4\n",
      "[5/5] Ha llegado alguien desconocido a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_05.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_05.mp3\n"
     ]
    }
   ],
   "source": [
    "def calibrate_clip_threshold_from_negative(\n",
    "    negative_video: Path,\n",
    "    conf: float = 0.25,\n",
    "    clip_margin: float = 0.15,\n",
    "    sample_every: int = 5,\n",
    "    max_frames: int = 200,\n",
    ") -> float:\n",
    "    \"\"\"Usa un vídeo NEGATIVO (sin repartidor) para fijar un umbral alto que evite falsos positivos.\n",
    "\n",
    "    Estrategia: calcula el máximo p_pos observado (con margen p_pos > p_neg + clip_margin) y fija\n",
    "    threshold = max_p_pos + 0.05 (cap a 0.95).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(negative_video))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"No se pudo abrir el vídeo: {negative_video}\")\n",
    "\n",
    "    max_pos = 0.0\n",
    "    frame_idx = 0\n",
    "    used = 0\n",
    "\n",
    "    try:\n",
    "        while used < max_frames:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            frame_idx += 1\n",
    "            if sample_every > 1 and (frame_idx % sample_every) != 0:\n",
    "                continue\n",
    "\n",
    "            # Detecta personas y mide p_pos (sin escribir vídeo)\n",
    "            h, w = frame.shape[:2]\n",
    "            res = yolo.predict(frame, conf=conf, verbose=False)[0]\n",
    "            if res.boxes is None or len(res.boxes) == 0:\n",
    "                used += 1\n",
    "                continue\n",
    "\n",
    "            for box_xyxy, cls_id in zip(res.boxes.xyxy.cpu().numpy(), res.boxes.cls.cpu().numpy()):\n",
    "                if int(cls_id) != COCO_PERSON_CLASS_ID:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = [int(v) for v in box_xyxy]\n",
    "                person = Box(x1, y1, x2, y2).clip(w, h)\n",
    "                if not person.valid():\n",
    "                    continue\n",
    "                crop = frame[person.y1 : person.y2, person.x1 : person.x2]\n",
    "                if crop.size == 0:\n",
    "                    continue\n",
    "                crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "                pil_person = Image.fromarray(crop_rgb)\n",
    "                p_pos, p_neg = clip_person_delivery_probs(pil_person)\n",
    "                if p_pos > p_neg + clip_margin:\n",
    "                    max_pos = max(max_pos, p_pos)\n",
    "\n",
    "            used += 1\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    thr = min(0.95, max(0.70, max_pos + 0.05))\n",
    "    print(f\"Calibración (negativo={negative_video.name}): max_p_pos={max_pos:.3f} -> clip_threshold={thr:.3f}\")\n",
    "    return thr\n",
    "\n",
    "\n",
    "def run_on_folder(\n",
    "    input_dir: Path = VIDEOS_IN,\n",
    "    output_dir: Path = VIDEOS_OUT,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: Optional[float] = None,\n",
    "    clip_margin: float = 0.15,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> None:\n",
    "    videos = sorted(input_dir.glob(\"*.mp4\"))\n",
    "    if not videos:\n",
    "        print(f\"No hay vídeos .mp4 en {input_dir}\")\n",
    "        return\n",
    "\n",
    "    # Según tu dataset: el primero NO es repartidor. Lo usamos para calibrar.\n",
    "    if clip_threshold is None:\n",
    "        clip_threshold = calibrate_clip_threshold_from_negative(\n",
    "            negative_video=videos[0],\n",
    "            conf=conf,\n",
    "            clip_margin=clip_margin,\n",
    "            sample_every=5,\n",
    "            max_frames=200,\n",
    "        )\n",
    "\n",
    "    for idx, in_path in enumerate(videos, start=1):\n",
    "        out_video = output_dir / f\"output_{in_path.name}\"\n",
    "        out_audio = output_dir / f\"output_{in_path.stem}.mp3\"\n",
    "\n",
    "        print(\"\\nProcesando:\", in_path.name)\n",
    "        delivery = process_video(\n",
    "            input_path=in_path,\n",
    "            output_path=out_video,\n",
    "            conf=conf,\n",
    "            clip_threshold=clip_threshold,\n",
    "            max_frames=max_frames,\n",
    "        )\n",
    "\n",
    "        if delivery:\n",
    "            msg_print = \"Ha llegado un repartidor a tu casa\"\n",
    "            msg_tts = \"Ha llegado un repartidor a tu domicilio. ¿Deseas abrirle?\"\n",
    "        else:\n",
    "            msg_print = \"Ha llegado alguien desconocido a tu casa\"\n",
    "            msg_tts = \"Ha llegado alguien desconocido a tu domicilio. ¿Deseas abrir?\"\n",
    "\n",
    "        print(f\"[{idx}/{len(videos)}] {msg_print}\")\n",
    "        tts_to_mp3(msg_tts, out_audio)\n",
    "        print(\"Vídeo salida:\", out_video)\n",
    "        print(\"Audio salida:\", out_audio)\n",
    "\n",
    "\n",
    "# Ejecuta el pipeline sobre todos los vídeos de entrada.\n",
    "run_on_folder(max_frames=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
