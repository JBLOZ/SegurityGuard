{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de Seguridad (Asistente Multimodal) — Vídeo ➜ Vídeo anotado + Voz\n",
    "\n",
    "## Objetivo\n",
    "Este notebook implementa un **prototipo de asistente para cámaras de seguridad** que procesa vídeos de entrada (`videos/entrada/*.mp4`) y, **al finalizar cada vídeo**:\n",
    "\n",
    "1. Genera un **vídeo de salida** en `videos/salida/` con las **personas recuadradas**.\n",
    "2. Clasifica el evento como:\n",
    "   - **Repartidor** si detecta que **alguna persona porta un paquete/caja**.\n",
    "   - **Desconocido** si **no** hay evidencia de paquete/caja.\n",
    "3. Muestra un **print** con la decisión:\n",
    "   - `Ha llegado un repartidor a tu casa`\n",
    "   - `Ha llegado alguien desconocido a tu casa`\n",
    "4. Genera un **audio (TTS)** `.mp3` con un mensaje equivalente:\n",
    "   - Repartidor: “Ha llegado un repartidor a tu domicilio. ¿Deseas abrirle?”\n",
    "   - Desconocido: “Ha llegado alguien desconocido a tu domicilio. ¿Deseas abrir?”\n",
    "\n",
    "## Cómo funciona el pipeline\n",
    "- **Detección (YOLOv8)**: en cada frame se detectan personas (`class=person`).\n",
    "- **Verificación de “paquete/caja” (CLIP)**: para cada persona detectada, se recorta la región de la persona y se aplica CLIP con un **clasificador binario**:\n",
    "  - positivo: “persona sosteniendo un paquete/caja”\n",
    "  - negativo: “persona sin nada en las manos”\n",
    "\n",
    "Esto se diseñó así para **reducir falsos positivos** (CLIP sobre recortes pequeños suele “imaginar” paquetes).\n",
    "\n",
    "## Calibración automática (anti-falsos-positivos)\n",
    "En nuestro data set nuestro primer video no contiene ningun repartidor por lo que lo que hacemos es usarlo como **negativo** para calibrar automáticamente un umbral de CLIP y evitar que aparezca “repartidor” donde no corresponde.\n",
    "\n",
    "## Entradas y salidas\n",
    "- **Entrada**: vídeos `.mp4` en `videos/entrada/`\n",
    "- **Salida**:\n",
    "  - `videos/salida/output_<nombre>.mp4` (vídeo anotado)\n",
    "  - `videos/salida/output_<nombre>.mp3` (audio TTS)\n",
    "\n",
    "## Notas\n",
    "- El sistema está pensado para ser simple y generalizable: funciona con cualquier vídeo donde aparezcan personas.\n",
    "- Puedes ajustar sensibilidad con `clip_threshold` y `clip_margin` en la función de ejecución del pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instalando: ultralytics>=8.0.0 opencv-python pillow numpy transformers>=4.35.0 torch torchvision gTTS protobuf>=5.28.0\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Setup (instalación de dependencias)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    \"ultralytics>=8.0.0\",\n",
    "    \"opencv-python\",\n",
    "    \"pillow\",\n",
    "    \"numpy\",\n",
    "    \"transformers>=4.35.0\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"gTTS\",\n",
    "    \"protobuf>=5.28.0\",\n",
    "]\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + list(pkgs)\n",
    "    print(\"Instalando:\", \" \".join(pkgs))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "pip_install(packages)\n",
    "print(\"OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "Input: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\entrada\n",
      "Output: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from gtts import gTTS\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "VIDEOS_IN = ROOT / \"videos\" / \"entrada\"\n",
    "VIDEOS_OUT = ROOT / \"videos\" / \"salida\"\n",
    "VIDEOS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "print(\"Input:\", VIDEOS_IN)\n",
    "print(\"Output:\", VIDEOS_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "c:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alema\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos cargados\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Carga de modelos\n",
    "\n",
    "# YOLOv8 (detección)\n",
    "yolo = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# CLIP (clasificación binaria sobre la persona)\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
    "\n",
    "COCO_PERSON_CLASS_ID = 0\n",
    "\n",
    "# Prompts binarios (reducen falsos positivos respecto a buscar \"caja\" en un recorte pequeño)\n",
    "CLIP_LABELS = [\n",
    "    \"una persona sosteniendo un paquete o una caja de cartón\",  # positivo\n",
    "    \"una persona sin nada en las manos\",                       # negativo\n",
    "]\n",
    "\n",
    "CLIP_POSITIVE_INDEX = 0\n",
    "CLIP_NEGATIVE_INDEX = 1\n",
    "\n",
    "print(\"Modelos cargados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Box:\n",
    "    x1: int\n",
    "    y1: int\n",
    "    x2: int\n",
    "    y2: int\n",
    "\n",
    "    def clip(self, w: int, h: int) -> \"Box\":\n",
    "        return Box(\n",
    "            x1=max(0, min(self.x1, w - 1)),\n",
    "            y1=max(0, min(self.y1, h - 1)),\n",
    "            x2=max(0, min(self.x2, w - 1)),\n",
    "            y2=max(0, min(self.y2, h - 1)),\n",
    "        )\n",
    "\n",
    "    def valid(self) -> bool:\n",
    "        return self.x2 > self.x1 and self.y2 > self.y1\n",
    "\n",
    "    def area(self) -> int:\n",
    "        if not self.valid():\n",
    "            return 0\n",
    "        return (self.x2 - self.x1) * (self.y2 - self.y1)\n",
    "\n",
    "\n",
    "def draw_box(\n",
    "    img_bgr: np.ndarray,\n",
    "    box: Box,\n",
    "    label: str,\n",
    "    color: Tuple[int, int, int],\n",
    "    thickness: int = 2,\n",
    ") -> None:\n",
    "    \"\"\"Dibuja un bounding box y una etiqueta sobre un frame (BGR).\n",
    "\n",
    "    Esta función se usa para generar el **vídeo de salida anotado**.\n",
    "\n",
    "    Args:\n",
    "        img_bgr: Imagen/frame en formato BGR (OpenCV).\n",
    "        box: Coordenadas (x1,y1,x2,y2) ya recortadas al frame.\n",
    "        label: Texto a mostrar (si es vacío, solo dibuja el rectángulo).\n",
    "        color: Color BGR del rectángulo y fondo de la etiqueta.\n",
    "        thickness: Grosor del rectángulo.\n",
    "    \"\"\"\n",
    "    cv2.rectangle(img_bgr, (box.x1, box.y1), (box.x2, box.y2), color, thickness)\n",
    "    if label:\n",
    "        (tw, th), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "        y = max(0, box.y1 - th - baseline - 4)\n",
    "        cv2.rectangle(img_bgr, (box.x1, y), (box.x1 + tw + 6, y + th + baseline + 6), color, -1)\n",
    "        cv2.putText(\n",
    "            img_bgr,\n",
    "            label,\n",
    "            (box.x1 + 3, y + th + 3),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (0, 0, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "\n",
    "def tts_to_mp3(text: str, out_path: Path, lang: str = \"es\") -> Path:\n",
    "    \"\"\"Genera un audio TTS (mp3) con el mensaje final del asistente.\n",
    "\n",
    "    Esta función se usa para producir la **salida de voz** al terminar cada vídeo.\n",
    "\n",
    "    Args:\n",
    "        text: Frase a sintetizar.\n",
    "        out_path: Ruta de salida del archivo `.mp3`.\n",
    "        lang: Idioma para gTTS (por defecto \"es\").\n",
    "\n",
    "    Returns:\n",
    "        La ruta donde se guardó el mp3.\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(str(out_path))\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def clip_person_delivery_probs(pil_person: Image.Image) -> Tuple[float, float]:\n",
    "    \"\"\"Devuelve (p_pos, p_neg) usando CLIP_LABELS binarios.\"\"\"\n",
    "    inputs = clip_processor(text=CLIP_LABELS, images=pil_person, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        probs = outputs.logits_per_image.softmax(dim=1)[0]\n",
    "\n",
    "    p_pos = float(probs[CLIP_POSITIVE_INDEX].detach().cpu().item())\n",
    "    p_neg = float(probs[CLIP_NEGATIVE_INDEX].detach().cpu().item())\n",
    "    return p_pos, p_neg\n",
    "\n",
    "\n",
    "def person_carry_region(person: Box, frame_w: int, frame_h: int) -> Box:\n",
    "    # Región auxiliar (solo para visualizar / debug si hace falta)\n",
    "    w = person.x2 - person.x1\n",
    "    h = person.y2 - person.y1\n",
    "\n",
    "    x1 = int(person.x1 - 0.10 * w)\n",
    "    x2 = int(person.x2 + 0.10 * w)\n",
    "    y1 = int(person.y1 + 0.15 * h)\n",
    "    y2 = int(person.y1 + 0.95 * h)\n",
    "\n",
    "    return Box(x1, y1, x2, y2).clip(frame_w, frame_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(\n",
    "    frame_bgr: np.ndarray,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: float = 0.80,\n",
    "    clip_margin: float = 0.15,\n",
    "    min_person_area_ratio: float = 0.03,\n",
    ") -> Tuple[np.ndarray, bool]:\n",
    "    \"\"\"Procesa un frame y devuelve el frame anotado + una bandera de 'repartidor'.\n",
    "\n",
    "    Esta función es la unidad básica para construir el **vídeo de salida**:\n",
    "    - Detecta personas con YOLO.\n",
    "    - Para cada persona, aplica CLIP binario sobre el recorte de la persona.\n",
    "    - Dibuja bounding boxes y etiquetas.\n",
    "\n",
    "    Args:\n",
    "        frame_bgr: Frame de entrada (BGR, OpenCV).\n",
    "        conf: Umbral de confianza para YOLO.\n",
    "        clip_threshold: Umbral mínimo de probabilidad para declarar \"repartidor\".\n",
    "        clip_margin: Margen adicional para exigir separación entre positivo y negativo.\n",
    "        min_person_area_ratio: Ignora detecciones de persona demasiado pequeñas.\n",
    "\n",
    "    Returns:\n",
    "        (frame_anotado, delivery_en_este_frame)\n",
    "\n",
    "    Notas:\n",
    "        - Este diseño prioriza **bajo falso positivo**.\n",
    "        - El evento final del vídeo se marca como repartidor si `delivery_en_este_frame` es True\n",
    "          en cualquier frame del clip.\n",
    "    \"\"\"\n",
    "    annotated = frame_bgr.copy()\n",
    "    h, w = annotated.shape[:2]\n",
    "\n",
    "    result = yolo.predict(frame_bgr, conf=conf, verbose=False)[0]\n",
    "    if result.boxes is None or len(result.boxes) == 0:\n",
    "        return annotated, False\n",
    "\n",
    "    delivery = False\n",
    "    min_person_area = int(min_person_area_ratio * (w * h))\n",
    "\n",
    "    for box_xyxy, cls_id, score in zip(\n",
    "        result.boxes.xyxy.cpu().numpy(),\n",
    "        result.boxes.cls.cpu().numpy(),\n",
    "        result.boxes.conf.cpu().numpy(),\n",
    "    ):\n",
    "        if int(cls_id) != COCO_PERSON_CLASS_ID:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = [int(v) for v in box_xyxy]\n",
    "        person = Box(x1, y1, x2, y2).clip(w, h)\n",
    "        if not person.valid() or person.area() < min_person_area:\n",
    "            continue\n",
    "\n",
    "        person_crop_bgr = frame_bgr[person.y1 : person.y2, person.x1 : person.x2]\n",
    "        if person_crop_bgr.size == 0:\n",
    "            continue\n",
    "\n",
    "        person_crop_rgb = cv2.cvtColor(person_crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_person = Image.fromarray(person_crop_rgb)\n",
    "\n",
    "        p_pos, p_neg = clip_person_delivery_probs(pil_person)\n",
    "\n",
    "        is_delivery = (p_pos >= clip_threshold) and (p_pos > p_neg + clip_margin)\n",
    "\n",
    "        if is_delivery:\n",
    "            delivery = True\n",
    "            draw_box(annotated, person, f\"REPARTIDOR {score:.2f} | {p_pos:.2f}\", (0, 0, 255), 3)\n",
    "        else:\n",
    "            draw_box(annotated, person, f\"persona {score:.2f} | {p_pos:.2f}\", (0, 200, 0), 2)\n",
    "\n",
    "    return annotated, delivery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(\n",
    "    input_path: Path,\n",
    "    output_path: Path,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: float = 0.80,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> bool:\n",
    "    \"\"\"Procesa un vídeo completo y genera el vídeo anotado de salida.\n",
    "\n",
    "    Lee frames del vídeo de entrada, aplica `process_frame(...)` a cada uno y escribe un\n",
    "    `.mp4` con las anotaciones (personas y etiqueta de \"REPARTIDOR\" cuando aplique).\n",
    "\n",
    "    Args:\n",
    "        input_path: Ruta al `.mp4` de entrada.\n",
    "        output_path: Ruta al `.mp4` anotado de salida.\n",
    "        conf: Umbral de confianza YOLO.\n",
    "        clip_threshold: Umbral de probabilidad CLIP para declarar repartidor.\n",
    "        max_frames: Si se indica, limita el número de frames procesados (útil para pruebas).\n",
    "\n",
    "    Returns:\n",
    "        True si se detectó repartidor en algún frame; False en caso contrario.\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(str(input_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"No se pudo abrir el vídeo: {input_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if not fps or fps <= 0:\n",
    "        fps = 25.0\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "\n",
    "    delivery_any = False\n",
    "    frame_idx = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            annotated, delivery = process_frame(\n",
    "                frame,\n",
    "                conf=conf,\n",
    "                clip_threshold=clip_threshold,\n",
    "            )\n",
    "            delivery_any = delivery_any or delivery\n",
    "\n",
    "            out.write(annotated)\n",
    "\n",
    "            frame_idx += 1\n",
    "            if max_frames is not None and frame_idx >= max_frames:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "    return delivery_any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando: video_01.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAudio salida:\u001b[39m\u001b[33m\"\u001b[39m, out_audio)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Ejecuta el pipeline sobre todos los vídeos de entrada.\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Consejo: para pruebas rápidas, usa max_frames=150.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mrun_on_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mrun_on_folder\u001b[39m\u001b[34m(input_dir, output_dir, conf, clip_threshold, max_frames)\u001b[39m\n\u001b[32m     15\u001b[39m out_audio = output_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutput_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_path.stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp3\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProcesando:\u001b[39m\u001b[33m\"\u001b[39m, in_path.name)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m delivery = \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_video\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m delivery:\n\u001b[32m     27\u001b[39m     msg_print = \u001b[33m\"\u001b[39m\u001b[33mHa llegado un repartidor a tu casa\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mprocess_video\u001b[39m\u001b[34m(input_path, output_path, conf, clip_threshold, max_frames)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m annotated, delivery, hay_personas = \u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m delivery_any = delivery_any \u001b[38;5;129;01mor\u001b[39;00m delivery\n\u001b[32m     56\u001b[39m hay_personas_any = hay_personas_any \u001b[38;5;129;01mor\u001b[39;00m hay_personas\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mprocess_frame\u001b[39m\u001b[34m(frame_bgr, conf, clip_threshold, clip_margin, min_person_area_ratio)\u001b[39m\n\u001b[32m     31\u001b[39m annotated = frame_bgr.copy()\n\u001b[32m     32\u001b[39m h, w = annotated.shape[:\u001b[32m2\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m result = \u001b[43myolo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_bgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result.boxes) == \u001b[32m0\u001b[39m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m annotated, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\engine\\model.py:535\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    534\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:225\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\utils\\_contextlib.py:38\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:330\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    332\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:182\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m visualize = (\n\u001b[32m    178\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    181\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:688\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    686\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:137\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:154\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:176\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    177\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:305\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    304\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    306\u001b[39m     y.extend(m(y[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:89\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     81\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     82\u001b[39m \n\u001b[32m     83\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alema\\anaconda3\\envs\\idspia\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def run_on_folder(\n",
    "    input_dir: Path = VIDEOS_IN,\n",
    "    output_dir: Path = VIDEOS_OUT,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: float = 0.35,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> None:\n",
    "    videos = sorted(input_dir.glob(\"*.mp4\"))\n",
    "    if not videos:\n",
    "        print(f\"No hay vídeos .mp4 en {input_dir}\")\n",
    "        return\n",
    "\n",
    "    for in_path in videos:\n",
    "        out_video = output_dir / f\"output_{in_path.name}\"\n",
    "        out_audio = output_dir / f\"output_{in_path.stem}.mp3\"\n",
    "\n",
    "        print(\"\\nProcesando:\", in_path.name)\n",
    "        delivery = process_video(\n",
    "            input_path=in_path,\n",
    "            output_path=out_video,\n",
    "            conf=conf,\n",
    "            clip_threshold=clip_threshold,\n",
    "            max_frames=max_frames,\n",
    "        )\n",
    "\n",
    "        if delivery:\n",
    "            msg_print = \"Ha llegado un repartidor a tu casa\"\n",
    "            msg_tts = \"Ha llegado un repartidor a tu domicilio. ¿Deseas abrirle?\"\n",
    "        else:\n",
    "            msg_print = \"Ha llegado alguien desconocido a tu casa\"\n",
    "            msg_tts = \"Ha llegado alguien desconocido a tu domicilio. ¿Deseas abrir?\"\n",
    "\n",
    "        print(msg_print)\n",
    "        tts_to_mp3(msg_tts, out_audio)\n",
    "        print(\"Vídeo salida:\", out_video)\n",
    "        print(\"Audio salida:\", out_audio)\n",
    "\n",
    "\n",
    "# Ejecuta el pipeline sobre todos los vídeos de entrada.\n",
    "# Consejo: para pruebas rápidas, usa max_frames=150.\n",
    "run_on_folder(max_frames=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibración (negativo=video_01.mp4): max_p_pos=0.937 -> clip_threshold=0.950\n",
      "\n",
      "Procesando: video_01.mp4\n",
      "[1/6] Ha llegado alguien desconocido a tu casa\n",
      "Vídeo salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_01.mp4\n",
      "Audio salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_01.mp3\n",
      "\n",
      "Procesando: video_02.mp4\n",
      "[2/6] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_02.mp4\n",
      "Audio salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_02.mp3\n",
      "\n",
      "Procesando: video_03.mp4\n",
      "[3/6] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_03.mp4\n",
      "Audio salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_03.mp3\n",
      "\n",
      "Procesando: video_04.mp4\n",
      "[4/6] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_04.mp4\n",
      "Audio salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_04.mp3\n",
      "\n",
      "Procesando: video_05.mp4\n",
      "[5/6] Ha llegado alguien desconocido a tu casa\n",
      "Vídeo salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_05.mp4\n",
      "Audio salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_05.mp3\n",
      "\n",
      "Procesando: video_06.mp4\n",
      "[6/6] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_06.mp4\n",
      "Audio salida: c:\\Users\\alema\\Documents\\Ingenieria IA\\TERCERO\\1 CUATRI\\persona-maquina\\SegurityGuard\\videos\\salida\\output_video_06.mp3\n"
     ]
    }
   ],
   "source": [
    "def calibrate_clip_threshold_from_negative(\n",
    "    negative_video: Path,\n",
    "    conf: float = 0.25,\n",
    "    clip_margin: float = 0.15,\n",
    "    sample_every: int = 5,\n",
    "    max_frames: int = 200,\n",
    ") -> float:\n",
    "    \"\"\"Calibra automáticamente `clip_threshold` usando un vídeo negativo (sin repartidor).\n",
    "\n",
    "    Objetivo: evitar falsos positivos. Si el vídeo NEGATIVO nunca debería disparar \"repartidor\",\n",
    "    medimos el máximo `p_pos` observado en ese vídeo y fijamos el umbral un poco por encima.\n",
    "\n",
    "    Args:\n",
    "        negative_video: Vídeo que asumimos que NO contiene repartidor.\n",
    "        conf: Umbral YOLO para encontrar personas.\n",
    "        clip_margin: Solo considera candidatos donde p_pos > p_neg + clip_margin.\n",
    "        sample_every: Procesa 1 de cada N frames (para acelerar).\n",
    "        max_frames: Máximo de frames muestreados.\n",
    "\n",
    "    Returns:\n",
    "        Un valor recomendado para `clip_threshold`.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(negative_video))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"No se pudo abrir el vídeo: {negative_video}\")\n",
    "\n",
    "    max_pos = 0.0\n",
    "    frame_idx = 0\n",
    "    used = 0\n",
    "\n",
    "    try:\n",
    "        while used < max_frames:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            frame_idx += 1\n",
    "            if sample_every > 1 and (frame_idx % sample_every) != 0:\n",
    "                continue\n",
    "\n",
    "            h, w = frame.shape[:2]\n",
    "            res = yolo.predict(frame, conf=conf, verbose=False)[0]\n",
    "            if res.boxes is None or len(res.boxes) == 0:\n",
    "                used += 1\n",
    "                continue\n",
    "\n",
    "            for box_xyxy, cls_id in zip(res.boxes.xyxy.cpu().numpy(), res.boxes.cls.cpu().numpy()):\n",
    "                if int(cls_id) != COCO_PERSON_CLASS_ID:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = [int(v) for v in box_xyxy]\n",
    "                person = Box(x1, y1, x2, y2).clip(w, h)\n",
    "                if not person.valid():\n",
    "                    continue\n",
    "                crop = frame[person.y1 : person.y2, person.x1 : person.x2]\n",
    "                if crop.size == 0:\n",
    "                    continue\n",
    "                crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "                pil_person = Image.fromarray(crop_rgb)\n",
    "                p_pos, p_neg = clip_person_delivery_probs(pil_person)\n",
    "                if p_pos > p_neg + clip_margin:\n",
    "                    max_pos = max(max_pos, p_pos)\n",
    "\n",
    "            used += 1\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    thr = min(0.95, max(0.70, max_pos + 0.05))\n",
    "    print(f\"Calibración (negativo={negative_video.name}): max_p_pos={max_pos:.3f} -> clip_threshold={thr:.3f}\")\n",
    "    return thr\n",
    "\n",
    "\n",
    "def run_on_folder(\n",
    "    input_dir: Path = VIDEOS_IN,\n",
    "    output_dir: Path = VIDEOS_OUT,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: Optional[float] = None,\n",
    "    clip_margin: float = 0.15,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> None:\n",
    "    \"\"\"Ejecuta el pipeline sobre todos los vídeos de una carpeta.\n",
    "\n",
    "    Para cada `.mp4` en `input_dir`:\n",
    "    - Genera un `.mp4` anotado en `output_dir`.\n",
    "    - Decide si es \"repartidor\" o \"desconocido\".\n",
    "    - Imprime el mensaje final.\n",
    "    - Genera un `.mp3` TTS con el mensaje.\n",
    "\n",
    "    Args:\n",
    "        input_dir: Carpeta con vídeos de entrada.\n",
    "        output_dir: Carpeta donde se guardan los vídeos y audios de salida.\n",
    "        conf: Umbral YOLO.\n",
    "        clip_threshold: Umbral CLIP para declarar repartidor. Si es None, se calibra con el primer vídeo.\n",
    "        clip_margin: Margen adicional p_pos > p_neg + clip_margin.\n",
    "        max_frames: Límite de frames (para pruebas).\n",
    "    \"\"\"\n",
    "    videos = sorted(input_dir.glob(\"*.mp4\"))\n",
    "    if not videos:\n",
    "        print(f\"No hay vídeos .mp4 en {input_dir}\")\n",
    "        return\n",
    "\n",
    "    if clip_threshold is None:\n",
    "        clip_threshold = calibrate_clip_threshold_from_negative(\n",
    "            negative_video=videos[0],\n",
    "            conf=conf,\n",
    "            clip_margin=clip_margin,\n",
    "            sample_every=5,\n",
    "            max_frames=200,\n",
    "        )\n",
    "\n",
    "    for idx, in_path in enumerate(videos, start=1):\n",
    "        out_video = output_dir / f\"output_{in_path.name}\"\n",
    "        out_audio = output_dir / f\"output_{in_path.stem}.mp3\"\n",
    "\n",
    "        print(\"\\nProcesando:\", in_path.name)\n",
    "        delivery = process_video(\n",
    "            input_path=in_path,\n",
    "            output_path=out_video,\n",
    "            conf=conf,\n",
    "            clip_threshold=clip_threshold,\n",
    "            max_frames=max_frames,\n",
    "        )\n",
    "\n",
    "        if delivery:\n",
    "            msg_print = \"Ha llegado un repartidor a tu casa\"\n",
    "            msg_tts = \"Ha llegado un repartidor a tu domicilio. ¿Deseas abrirle?\"\n",
    "        else:\n",
    "            msg_print = \"Ha llegado alguien desconocido a tu casa\"\n",
    "            msg_tts = \"Ha llegado alguien desconocido a tu domicilio. ¿Deseas abrir?\"\n",
    "\n",
    "        print(f\"[{idx}/{len(videos)}] {msg_print}\")\n",
    "        tts_to_mp3(msg_tts, out_audio)\n",
    "        print(\"Vídeo salida:\", out_video)\n",
    "        print(\"Audio salida:\", out_audio)\n",
    "\n",
    "\n",
    "# Ejecuta el pipeline sobre todos los vídeos de entrada.\n",
    "run_on_folder(max_frames=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idspia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
