{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de Seguridad (Asistente Multimodal) — Vídeo ➜ Vídeo anotado + Voz\n",
    "\n",
    "## Objetivo\n",
    "Este notebook implementa un **prototipo de asistente para cámaras de seguridad** que procesa vídeos de entrada (`videos/entrada/*.mp4`) y, **al finalizar cada vídeo**:\n",
    "\n",
    "1. Genera un **vídeo de salida** en `videos/salida/` con las **personas recuadradas**.\n",
    "2. Clasifica el evento como:\n",
    "   - **Repartidor** si detecta que **alguna persona porta un paquete/caja**.\n",
    "   - **Desconocido** si **no** hay evidencia de paquete/caja.\n",
    "3. Muestra un **print** con la decisión:\n",
    "   - `Ha llegado un repartidor a tu casa`\n",
    "   - `Ha llegado alguien desconocido a tu casa`\n",
    "4. Genera un **audio (TTS)** `.mp3` con un mensaje equivalente:\n",
    "   - Repartidor: “Ha llegado un repartidor a tu domicilio. ¿Deseas abrirle?”\n",
    "   - Desconocido: “Ha llegado alguien desconocido a tu domicilio. ¿Deseas abrir?”\n",
    "\n",
    "## Cómo funciona el pipeline\n",
    "- **Detección (YOLOv8)**: en cada frame se detectan personas (`class=person`).\n",
    "- **Verificación de “paquete/caja” (CLIP)**: para cada persona detectada, se recorta la región de la persona y se aplica CLIP con un **clasificador binario**:\n",
    "  - positivo: “persona sosteniendo un paquete/caja”\n",
    "  - negativo: “persona sin nada en las manos”\n",
    "\n",
    "Esto se diseñó así para **reducir falsos positivos** (CLIP sobre recortes pequeños suele “imaginar” paquetes).\n",
    "\n",
    "## Calibración automática (anti-falsos-positivos)\n",
    "En nuestro data set nuestro primer video no contiene ningun repartidor por lo que lo que hacemos es usarlo como **negativo** para calibrar automáticamente un umbral de CLIP y evitar que aparezca “repartidor” donde no corresponde.\n",
    "\n",
    "## Entradas y salidas\n",
    "- **Entrada**: vídeos `.mp4` en `videos/entrada/`\n",
    "- **Salida**:\n",
    "  - `videos/salida/output_<nombre>.mp4` (vídeo anotado)\n",
    "  - `videos/salida/output_<nombre>.mp3` (audio TTS)\n",
    "\n",
    "## Notas\n",
    "- El sistema está pensado para ser simple y generalizable: funciona con cualquier vídeo donde aparezcan personas.\n",
    "- Puedes ajustar sensibilidad con `clip_threshold` y `clip_margin` en la función de ejecución del pipeline.\n",
    "- Para ejecutar el código con nuestros videos de prueba o simplemente ver nuestros videos y audios generados clonar el siguiente repositorio: https://github.com/JBLOZ/SegurityGuard.git\n",
    "|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instalando: ultralytics>=8.0.0 opencv-python pillow numpy transformers>=4.35.0 torch torchvision gTTS protobuf>=5.28.0\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Setup (instalación de dependencias)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    \"ultralytics>=8.0.0\",\n",
    "    \"opencv-python\",\n",
    "    \"pillow\",\n",
    "    \"numpy\",\n",
    "    \"transformers>=4.35.0\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"gTTS\",\n",
    "    \"protobuf>=5.28.0\",\n",
    "]\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + list(pkgs)\n",
    "    print(\"Instalando:\", \" \".join(pkgs))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "pip_install(packages)\n",
    "print(\"OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Input: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\entrada\n",
      "Output: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from gtts import gTTS\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "VIDEOS_IN = ROOT / \"videos\" / \"entrada\"\n",
    "VIDEOS_OUT = ROOT / \"videos\" / \"salida\"\n",
    "VIDEOS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "print(\"Input:\", VIDEOS_IN)\n",
    "print(\"Output:\", VIDEOS_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos cargados\n"
     ]
    }
   ],
   "source": [
    "# Carga de modelos\n",
    "\n",
    "# YOLOv8 (detección)\n",
    "yolo = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# CLIP (clasificación binaria sobre la persona)\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
    "\n",
    "COCO_PERSON_CLASS_ID = 0\n",
    "\n",
    "# Prompts binarios (reducen falsos positivos respecto a buscar \"caja\" en un recorte pequeño)\n",
    "CLIP_LABELS = [\n",
    "    \"una persona sosteniendo un paquete o una caja de cartón\",  # positivo\n",
    "    \"una persona sin nada en las manos\",                       # negativo\n",
    "]\n",
    "\n",
    "CLIP_POSITIVE_INDEX = 0\n",
    "CLIP_NEGATIVE_INDEX = 1\n",
    "\n",
    "print(\"Modelos cargados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Box:\n",
    "    x1: int\n",
    "    y1: int\n",
    "    x2: int\n",
    "    y2: int\n",
    "\n",
    "    def clip(self, w: int, h: int) -> \"Box\":\n",
    "        return Box(\n",
    "            x1=max(0, min(self.x1, w - 1)),\n",
    "            y1=max(0, min(self.y1, h - 1)),\n",
    "            x2=max(0, min(self.x2, w - 1)),\n",
    "            y2=max(0, min(self.y2, h - 1)),\n",
    "        )\n",
    "\n",
    "    def valid(self) -> bool:\n",
    "        return self.x2 > self.x1 and self.y2 > self.y1\n",
    "\n",
    "    def area(self) -> int:\n",
    "        if not self.valid():\n",
    "            return 0\n",
    "        return (self.x2 - self.x1) * (self.y2 - self.y1)\n",
    "\n",
    "\n",
    "def draw_box(\n",
    "    img_bgr: np.ndarray,\n",
    "    box: Box,\n",
    "    label: str,\n",
    "    color: Tuple[int, int, int],\n",
    "    thickness: int = 2,\n",
    ") -> None:\n",
    "    \"\"\"Dibuja un bounding box y una etiqueta sobre un frame (BGR).\n",
    "\n",
    "    Esta función se usa para generar el **vídeo de salida anotado**.\n",
    "\n",
    "    Args:\n",
    "        img_bgr: Imagen/frame en formato BGR (OpenCV).\n",
    "        box: Coordenadas (x1,y1,x2,y2) ya recortadas al frame.\n",
    "        label: Texto a mostrar (si es vacío, solo dibuja el rectángulo).\n",
    "        color: Color BGR del rectángulo y fondo de la etiqueta.\n",
    "        thickness: Grosor del rectángulo.\n",
    "    \"\"\"\n",
    "    cv2.rectangle(img_bgr, (box.x1, box.y1), (box.x2, box.y2), color, thickness)\n",
    "    if label:\n",
    "        (tw, th), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "        y = max(0, box.y1 - th - baseline - 4)\n",
    "        cv2.rectangle(img_bgr, (box.x1, y), (box.x1 + tw + 6, y + th + baseline + 6), color, -1)\n",
    "        cv2.putText(\n",
    "            img_bgr,\n",
    "            label,\n",
    "            (box.x1 + 3, y + th + 3),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (0, 0, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "\n",
    "def tts_to_mp3(text: str, out_path: Path, lang: str = \"es\") -> Path:\n",
    "    \"\"\"Genera un audio TTS (mp3) con el mensaje final del asistente.\n",
    "\n",
    "    Esta función se usa para producir la **salida de voz** al terminar cada vídeo.\n",
    "\n",
    "    Args:\n",
    "        text: Frase a sintetizar.\n",
    "        out_path: Ruta de salida del archivo `.mp3`.\n",
    "        lang: Idioma para gTTS (por defecto \"es\").\n",
    "\n",
    "    Returns:\n",
    "        La ruta donde se guardó el mp3.\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(str(out_path))\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def clip_person_delivery_probs(pil_person: Image.Image) -> Tuple[float, float]:\n",
    "    \"\"\"Devuelve (p_pos, p_neg) usando CLIP_LABELS binarios.\"\"\"\n",
    "    inputs = clip_processor(text=CLIP_LABELS, images=pil_person, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        probs = outputs.logits_per_image.softmax(dim=1)[0]\n",
    "\n",
    "    p_pos = float(probs[CLIP_POSITIVE_INDEX].detach().cpu().item())\n",
    "    p_neg = float(probs[CLIP_NEGATIVE_INDEX].detach().cpu().item())\n",
    "    return p_pos, p_neg\n",
    "\n",
    "\n",
    "def person_carry_region(person: Box, frame_w: int, frame_h: int) -> Box:\n",
    "    # Región auxiliar (solo para visualizar / debug si hace falta)\n",
    "    w = person.x2 - person.x1\n",
    "    h = person.y2 - person.y1\n",
    "\n",
    "    x1 = int(person.x1 - 0.10 * w)\n",
    "    x2 = int(person.x2 + 0.10 * w)\n",
    "    y1 = int(person.y1 + 0.15 * h)\n",
    "    y2 = int(person.y1 + 0.95 * h)\n",
    "\n",
    "    return Box(x1, y1, x2, y2).clip(frame_w, frame_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(\n",
    "    frame_bgr: np.ndarray,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: float = 0.80,\n",
    "    clip_margin: float = 0.15,\n",
    "    min_person_area_ratio: float = 0.03,\n",
    ") -> Tuple[np.ndarray, bool]:\n",
    "    \"\"\"Procesa un frame y devuelve el frame anotado + una bandera de 'repartidor'.\n",
    "\n",
    "    Esta función es la unidad básica para construir el **vídeo de salida**:\n",
    "    - Detecta personas con YOLO.\n",
    "    - Para cada persona, aplica CLIP binario sobre el recorte de la persona.\n",
    "    - Dibuja bounding boxes y etiquetas.\n",
    "\n",
    "    Args:\n",
    "        frame_bgr: Frame de entrada (BGR, OpenCV).\n",
    "        conf: Umbral de confianza para YOLO.\n",
    "        clip_threshold: Umbral mínimo de probabilidad para declarar \"repartidor\".\n",
    "        clip_margin: Margen adicional para exigir separación entre positivo y negativo.\n",
    "        min_person_area_ratio: Ignora detecciones de persona demasiado pequeñas.\n",
    "\n",
    "    Returns:\n",
    "        (frame_anotado, delivery_en_este_frame)\n",
    "\n",
    "    Notas:\n",
    "        - Este diseño prioriza **bajo falso positivo**.\n",
    "        - El evento final del vídeo se marca como repartidor si `delivery_en_este_frame` es True\n",
    "          en cualquier frame del clip.\n",
    "    \"\"\"\n",
    "    annotated = frame_bgr.copy()\n",
    "    h, w = annotated.shape[:2]\n",
    "\n",
    "    result = yolo.predict(frame_bgr, conf=conf, verbose=False)[0]\n",
    "    if result.boxes is None or len(result.boxes) == 0:\n",
    "        return annotated, False\n",
    "\n",
    "    delivery = False\n",
    "    min_person_area = int(min_person_area_ratio * (w * h))\n",
    "\n",
    "    for box_xyxy, cls_id, score in zip(\n",
    "        result.boxes.xyxy.cpu().numpy(),\n",
    "        result.boxes.cls.cpu().numpy(),\n",
    "        result.boxes.conf.cpu().numpy(),\n",
    "    ):\n",
    "        if int(cls_id) != COCO_PERSON_CLASS_ID:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = [int(v) for v in box_xyxy]\n",
    "        person = Box(x1, y1, x2, y2).clip(w, h)\n",
    "        if not person.valid() or person.area() < min_person_area:\n",
    "            continue\n",
    "\n",
    "        person_crop_bgr = frame_bgr[person.y1 : person.y2, person.x1 : person.x2]\n",
    "        if person_crop_bgr.size == 0:\n",
    "            continue\n",
    "\n",
    "        person_crop_rgb = cv2.cvtColor(person_crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_person = Image.fromarray(person_crop_rgb)\n",
    "\n",
    "        p_pos, p_neg = clip_person_delivery_probs(pil_person)\n",
    "\n",
    "        is_delivery = (p_pos >= clip_threshold) and (p_pos > p_neg + clip_margin)\n",
    "\n",
    "        if is_delivery:\n",
    "            delivery = True\n",
    "            draw_box(annotated, person, f\"REPARTIDOR {score:.2f} | {p_pos:.2f}\", (0, 0, 255), 3)\n",
    "        else:\n",
    "            draw_box(annotated, person, f\"persona {score:.2f} | {p_pos:.2f}\", (0, 200, 0), 2)\n",
    "\n",
    "    return annotated, delivery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(\n",
    "    input_path: Path,\n",
    "    output_path: Path,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: float = 0.80,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> bool:\n",
    "    \"\"\"Procesa un vídeo completo y genera el vídeo anotado de salida.\n",
    "\n",
    "    Lee frames del vídeo de entrada, aplica `process_frame(...)` a cada uno y escribe un\n",
    "    `.mp4` con las anotaciones (personas y etiqueta de \"REPARTIDOR\" cuando aplique).\n",
    "\n",
    "    Args:\n",
    "        input_path: Ruta al `.mp4` de entrada.\n",
    "        output_path: Ruta al `.mp4` anotado de salida.\n",
    "        conf: Umbral de confianza YOLO.\n",
    "        clip_threshold: Umbral de probabilidad CLIP para declarar repartidor.\n",
    "        max_frames: Si se indica, limita el número de frames procesados (útil para pruebas).\n",
    "\n",
    "    Returns:\n",
    "        True si se detectó repartidor en algún frame; False en caso contrario.\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(str(input_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"No se pudo abrir el vídeo: {input_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if not fps or fps <= 0:\n",
    "        fps = 25.0\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "\n",
    "    delivery_any = False\n",
    "    frame_idx = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            annotated, delivery = process_frame(\n",
    "                frame,\n",
    "                conf=conf,\n",
    "                clip_threshold=clip_threshold,\n",
    "            )\n",
    "            delivery_any = delivery_any or delivery\n",
    "\n",
    "            out.write(annotated)\n",
    "\n",
    "            frame_idx += 1\n",
    "            if max_frames is not None and frame_idx >= max_frames:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "    return delivery_any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando: video_01.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_01.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_01.mp3\n",
      "\n",
      "Procesando: video_02.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_02.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_02.mp3\n",
      "\n",
      "Procesando: video_03.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_03.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_03.mp3\n",
      "\n",
      "Procesando: video_04.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_04.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_04.mp3\n",
      "\n",
      "Procesando: video_05.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_05.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_05.mp3\n",
      "\n",
      "Procesando: video_06.mp4\n",
      "Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_06.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_06.mp3\n"
     ]
    }
   ],
   "source": [
    "def run_on_folder(\n",
    "    input_dir: Path = VIDEOS_IN,\n",
    "    output_dir: Path = VIDEOS_OUT,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: float = 0.35,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> None:\n",
    "    videos = sorted(input_dir.glob(\"*.mp4\"))\n",
    "    if not videos:\n",
    "        print(f\"No hay vídeos .mp4 en {input_dir}\")\n",
    "        return\n",
    "\n",
    "    for in_path in videos:\n",
    "        out_video = output_dir / f\"output_{in_path.name}\"\n",
    "        out_audio = output_dir / f\"output_{in_path.stem}.mp3\"\n",
    "\n",
    "        print(\"\\nProcesando:\", in_path.name)\n",
    "        delivery = process_video(\n",
    "            input_path=in_path,\n",
    "            output_path=out_video,\n",
    "            conf=conf,\n",
    "            clip_threshold=clip_threshold,\n",
    "            max_frames=max_frames,\n",
    "        )\n",
    "\n",
    "        if delivery:\n",
    "            msg_print = \"Ha llegado un repartidor a tu casa\"\n",
    "            msg_tts = \"Ha llegado un repartidor a tu domicilio. ¿Deseas abrirle?\"\n",
    "        else:\n",
    "            msg_print = \"Ha llegado alguien desconocido a tu casa\"\n",
    "            msg_tts = \"Ha llegado alguien desconocido a tu domicilio. ¿Deseas abrir?\"\n",
    "\n",
    "        print(msg_print)\n",
    "        tts_to_mp3(msg_tts, out_audio)\n",
    "        print(\"Vídeo salida:\", out_video)\n",
    "        print(\"Audio salida:\", out_audio)\n",
    "\n",
    "\n",
    "# Ejecuta el pipeline sobre todos los vídeos de entrada.\n",
    "# Consejo: para pruebas rápidas, usa max_frames=150.\n",
    "run_on_folder(max_frames=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibración (negativo=video_01.mp4): max_p_pos=0.937 -> clip_threshold=0.950\n",
      "\n",
      "Procesando: video_01.mp4\n",
      "[1/6] Ha llegado alguien desconocido a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_01.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_01.mp3\n",
      "\n",
      "Procesando: video_02.mp4\n",
      "[2/6] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_02.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_02.mp3\n",
      "\n",
      "Procesando: video_03.mp4\n",
      "[3/6] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_03.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_03.mp3\n",
      "\n",
      "Procesando: video_04.mp4\n",
      "[4/6] Ha llegado un repartidor a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_04.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_04.mp3\n",
      "\n",
      "Procesando: video_05.mp4\n",
      "[5/6] Ha llegado alguien desconocido a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_05.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_05.mp3\n",
      "\n",
      "Procesando: video_06.mp4\n",
      "[6/6] Ha llegado alguien desconocido a tu casa\n",
      "Vídeo salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_06.mp4\n",
      "Audio salida: c:\\Users\\jordi\\Documents\\UNI\\SegurityGuard\\videos\\salida\\output_video_06.mp3\n"
     ]
    }
   ],
   "source": [
    "def calibrate_clip_threshold_from_negative(\n",
    "    negative_video: Path,\n",
    "    conf: float = 0.25,\n",
    "    clip_margin: float = 0.15,\n",
    "    sample_every: int = 5,\n",
    "    max_frames: int = 200,\n",
    ") -> float:\n",
    "    \"\"\"Calibra automáticamente `clip_threshold` usando un vídeo negativo (sin repartidor).\n",
    "\n",
    "    Objetivo: evitar falsos positivos. Si el vídeo NEGATIVO nunca debería disparar \"repartidor\",\n",
    "    medimos el máximo `p_pos` observado en ese vídeo y fijamos el umbral un poco por encima.\n",
    "\n",
    "    Args:\n",
    "        negative_video: Vídeo que asumimos que NO contiene repartidor.\n",
    "        conf: Umbral YOLO para encontrar personas.\n",
    "        clip_margin: Solo considera candidatos donde p_pos > p_neg + clip_margin.\n",
    "        sample_every: Procesa 1 de cada N frames (para acelerar).\n",
    "        max_frames: Máximo de frames muestreados.\n",
    "\n",
    "    Returns:\n",
    "        Un valor recomendado para `clip_threshold`.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(negative_video))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"No se pudo abrir el vídeo: {negative_video}\")\n",
    "\n",
    "    max_pos = 0.0\n",
    "    frame_idx = 0\n",
    "    used = 0\n",
    "\n",
    "    try:\n",
    "        while used < max_frames:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            frame_idx += 1\n",
    "            if sample_every > 1 and (frame_idx % sample_every) != 0:\n",
    "                continue\n",
    "\n",
    "            h, w = frame.shape[:2]\n",
    "            res = yolo.predict(frame, conf=conf, verbose=False)[0]\n",
    "            if res.boxes is None or len(res.boxes) == 0:\n",
    "                used += 1\n",
    "                continue\n",
    "\n",
    "            for box_xyxy, cls_id in zip(res.boxes.xyxy.cpu().numpy(), res.boxes.cls.cpu().numpy()):\n",
    "                if int(cls_id) != COCO_PERSON_CLASS_ID:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = [int(v) for v in box_xyxy]\n",
    "                person = Box(x1, y1, x2, y2).clip(w, h)\n",
    "                if not person.valid():\n",
    "                    continue\n",
    "                crop = frame[person.y1 : person.y2, person.x1 : person.x2]\n",
    "                if crop.size == 0:\n",
    "                    continue\n",
    "                crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "                pil_person = Image.fromarray(crop_rgb)\n",
    "                p_pos, p_neg = clip_person_delivery_probs(pil_person)\n",
    "                if p_pos > p_neg + clip_margin:\n",
    "                    max_pos = max(max_pos, p_pos)\n",
    "\n",
    "            used += 1\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    thr = min(0.95, max(0.70, max_pos + 0.05))\n",
    "    print(f\"Calibración (negativo={negative_video.name}): max_p_pos={max_pos:.3f} -> clip_threshold={thr:.3f}\")\n",
    "    return thr\n",
    "\n",
    "\n",
    "def run_on_folder(\n",
    "    input_dir: Path = VIDEOS_IN,\n",
    "    output_dir: Path = VIDEOS_OUT,\n",
    "    conf: float = 0.25,\n",
    "    clip_threshold: Optional[float] = None,\n",
    "    clip_margin: float = 0.15,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> None:\n",
    "    \"\"\"Ejecuta el pipeline sobre todos los vídeos de una carpeta.\n",
    "\n",
    "    Para cada `.mp4` en `input_dir`:\n",
    "    - Genera un `.mp4` anotado en `output_dir`.\n",
    "    - Decide si es \"repartidor\" o \"desconocido\".\n",
    "    - Imprime el mensaje final.\n",
    "    - Genera un `.mp3` TTS con el mensaje.\n",
    "\n",
    "    Args:\n",
    "        input_dir: Carpeta con vídeos de entrada.\n",
    "        output_dir: Carpeta donde se guardan los vídeos y audios de salida.\n",
    "        conf: Umbral YOLO.\n",
    "        clip_threshold: Umbral CLIP para declarar repartidor. Si es None, se calibra con el primer vídeo.\n",
    "        clip_margin: Margen adicional p_pos > p_neg + clip_margin.\n",
    "        max_frames: Límite de frames (para pruebas).\n",
    "    \"\"\"\n",
    "    videos = sorted(input_dir.glob(\"*.mp4\"))\n",
    "    if not videos:\n",
    "        print(f\"No hay vídeos .mp4 en {input_dir}\")\n",
    "        return\n",
    "\n",
    "    if clip_threshold is None:\n",
    "        clip_threshold = calibrate_clip_threshold_from_negative(\n",
    "            negative_video=videos[0],\n",
    "            conf=conf,\n",
    "            clip_margin=clip_margin,\n",
    "            sample_every=5,\n",
    "            max_frames=200,\n",
    "        )\n",
    "\n",
    "    for idx, in_path in enumerate(videos, start=1):\n",
    "        out_video = output_dir / f\"output_{in_path.name}\"\n",
    "        out_audio = output_dir / f\"output_{in_path.stem}.mp3\"\n",
    "\n",
    "        print(\"\\nProcesando:\", in_path.name)\n",
    "        delivery = process_video(\n",
    "            input_path=in_path,\n",
    "            output_path=out_video,\n",
    "            conf=conf,\n",
    "            clip_threshold=clip_threshold,\n",
    "            max_frames=max_frames,\n",
    "        )\n",
    "\n",
    "        if delivery:\n",
    "            msg_print = \"Ha llegado un repartidor a tu casa\"\n",
    "            msg_tts = \"Ha llegado un repartidor a tu domicilio. ¿Deseas abrirle?\"\n",
    "        else:\n",
    "            msg_print = \"Ha llegado alguien desconocido a tu casa\"\n",
    "            msg_tts = \"Ha llegado alguien desconocido a tu domicilio. ¿Deseas abrir?\"\n",
    "\n",
    "        print(f\"[{idx}/{len(videos)}] {msg_print}\")\n",
    "        tts_to_mp3(msg_tts, out_audio)\n",
    "        print(\"Vídeo salida:\", out_video)\n",
    "        print(\"Audio salida:\", out_audio)\n",
    "\n",
    "\n",
    "# Ejecuta el pipeline sobre todos los vídeos de entrada.\n",
    "run_on_folder(max_frames=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
